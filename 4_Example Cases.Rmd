
# Example Cases

## 1-D toy Problem

In this section, a 1-D toy problem is considered to illkuystrate the Bayes Optimization workflow dicussed in the previous section. The 1-D problem was selected sinnce it will help to visuluze all the steps of the workslow making easier explanation of the concepts. Though, it it can bee senn from the next section, the workflow can easily extened to to higer dimetional problem. The *True function* to be optimized in this section has an analytical expression as, given the box constrains:

```{=tex}
\begin{equation}
\begin{aligned}
& \underset{x}{\text{maximize}}
& & f(x) = 1-\frac{1}{2}(\frac{\sin (12x)}{1+x} + 2\cos(7x)x^5 + 0.7)  \\
& \text{subject to}
& & 0 \leq x \leq 1
\end{aligned}
\label{eq:1deq}
\end{equation}
```
Since the analytical expression of function available and being 1-D problem, the global optimum of the function had been found at the $x_M = 3.90$. The plot of the function and the optimum point has been shown in the Figure \@ref(fig:onedplot)

```{r loaddata, cache=TRUE, message=FALSE, error=FALSE, warning=FALSE, echo=FALSE}
bo_12345 <- readRDS("processed_data/bo_12345_401000.Rds")
#
bo_1234 <- readRDS("processed_data/bo_1234_401000.Rds")
#
bo_123 <- readRDS("processed_data/bo_123_401000.Rds")

#######################BO_50 ############################
bo_123_50 <- readRDS("processed_data/bo_1234_50_10000.Rds")
ss_50 <- bo_123_50$scoreSummary

########### GA ######################################

ga_12345 <- readRDS("processed_data/GA_12345.Rds")
#
ga_1234 <- readRDS("processed_data/GA_1234.Rds")
#
ga_123 <- readRDS("processed_data/GA_123.Rds")

########### PSO #######################################
#
pso_12345 <- readRDS("processed_data/pso_12345.Rds")
#
pso_1234 <- readRDS("processed_data/pso_1234.Rds")
#
pso_123 <- readRDS("processed_data/pso_123.Rds")
```

```{r loadlibraries, message=FALSE, echo=FALSE, error=FALSE}

one_d_fun <- function(x) {
  y <- (1-1/2*((sin(12*x)/(1+x))+(2*cos(7*x)*x^5)+0.7))
  return(y)
}

xmin <- optimize(one_d_fun, c(0, 1), tol = 0.0001, maximum = TRUE)

library(DiceKriging)
library(mvtnorm)
library(tidyverse)

library(tibble)
library(kableExtra)

library(tidyverse)
library(latex2exp)

library(patchwork)
library(gridExtra)
library(tidyverse)

```

```{r onedplot, echo=FALSE, fig.retina=2, fig.align='center', fig.cap="Plot of 1-D equation with blue dash line representing the global optimum", warning=FALSE, out.width="90%"}

x_domain <- seq(0,1,0.01)
y_domain <- one_d_fun(x_domain)

data_domain <- tibble(x=x_domain, y= y_domain)

df_text <- data.frame(
  x = 0.39,
  y = 0,
  text = c("bottom-left")
)
ggplot(data_domain, aes(x,y)) +
  geom_point() +
  geom_vline(xintercept = 0.390, linetype="dotted", 
                color = "blue", size=1.5) +
   annotate("text", x=0.32, y=0, label= "x_M", hjust = 0, vjust=+1 ,colour = "blue") +
  ylim(0,1.04) +
  ylab("f(x)")

```

However, it is worth to mention that the analytical expression of objective function in many of real world problems are not avilable, what is avilable is a *samples* from the objective function. Thefore, in the coming example a few samples are sequentiall drawn from the objective function to resemble the real case scenario. However, we know the global optimum of the objective function in hindsight, just in the case we want to copare the performace of Bayesian optimisation algorithem.

Thefore, as Figure \@ref(fig:exampleshow), the 5 sample points, $x=[0.05,0.2,0.5,0.6,0.95]$ were selected as the initialization of the workflow. In the upper plot, blue lines represnets the samples from posterior of the gaussian model conditioned on the five sample points. The grey area represents the 95% confidence interval while the red curve represents the mean value of the samples (blue lines). The first point to infer from the Figure \@ref(fig:exampleshow) is there no uncertainty on the sample point. As shown, there is no grey zone on sample point since as was dicussed in the previous section, here we consider the "noise-free" observation. Also, worth to mention that we have wide more uncertainity (wider grey band) in the reas that are more distant from the observation, simply meaning we are less uncertain close to observation points. on the "extrpolation", meaning in the ares outseide of the observation points, the probalistic model shows inetrsting behaviour. on those "exterme" area, the mean curve tend to move toward the mean of all observation points , here around 0, showing the model refelctes the mean-revervion behaviour when it comes extrpolation.

The lower part of Figure \@ref(fig:exampleshow), shows the plot of utility function at each x values. Worth to note that as the plot suggest, the utility($\alpha_{EI}$) function will have the muti-modal structure, meaning in the optimization process multi-start gradient method will be helpful, in order to avoid stock in the local optima. In this work, as was explained in the preious section, the multi-start gradient method was used. The blue dotted line line shows the the $x_{next}$ which is the point where the utility function, is maximum. Then this $x_{next}$ is qured from the real function, and the the pair of $(x_{next}, f(x_{next}))$ is added to the intial data set, $\mathcal{D}$. Going back to the lower figure at Figure \@ref(fig:exampleshow), the utility has two mode around point $x=0.5$, say $x_{0.5}^+$ and $x_{0.5}^-$, however the point $x_{0.5}^-$ is selected as the next query point. Readers can be rfereed to the upper plot and it is clear that there is more uncertainity around point $x_{0.5}^-$ than $x_{0.5}^+$ which given the form of utility function, that is understandable. The utility function always looking for the point that not only maximize the mean value, but also intereded in the points that has higher variannce, which is the case between two points $x_{0.5}^+$ and $x_{0.5}^-$.

```{r echo=FALSE, message=FALSE}
set.seed(123)
# ######################################################
# one_d_fun <- function(x) {
#   y <- (1-1/2*((sin(12*x)/(1+x))+(2*cos(7*x)*x^5)+0.7))
#   return(y)
# }
# #################################################
# 
# xmin <- optimize(one_d_fun, c(0, 1), tol = 0.0001, maximum = TRUE)
#########################################################

x_domain <- seq(0,1,0.01)
y_domain <- one_d_fun(x_domain)
#y_domain <- max(y_domain) - min(y_domain)
data_domain <- tibble(x=x_domain, y= y_domain)
#data_domain

#r <- max(vec) - min(vec)
#vec <- (vec - min(vec))/r
#########################################################
obs_data_return <- function(x) {
  y_norm <- one_d_fun(x)
  #y_norm = y-mean(y)
  df <- data.frame(x,y_norm)
}
#################################

km_model <- function(obs_data, predict_x) {
  
  model <- km(~0, design = data.frame(x=obs_data$x), response = obs_data$y_norm, multistart = 100, 
              control =list(trace=FALSE))
  paste0(model@covariance@range.val)
  p.SK <- predict(model, newdata=data.frame(x=predict_x), type="SK",cov.compute = TRUE)
  return(list(predict_list=p.SK,cov_par=model@covariance@range.val))
}

###################################

plot_post <- function(predict_list,x_predict,obs_data) {
  
  mv_sample <- mvtnorm::rmvnorm(100, predict_list$mean, predict_list$cov)
  ss <- t(mv_sample)
  
  dat <-data.frame(x=x_predict, ss) %>% 
    pivot_longer(-x, names_to = "rep", values_to = "value") %>% 
    mutate(rep=as.numeric(as.factor(rep)))
  
  data_gp <- data.frame(x=x_predict,upper95=predict_list$upper95,
                        lower95=predict_list$lower95, mean_curve=predict_list$mean)
  
  
  ggplot(dat,aes(x=x,y=value)) + 
    geom_line(aes(group=as.factor(rep), color="blue"), alpha=0.7) +
    #scale_colour_manual("",values = cols) +
    scale_color_manual("", values = c("black","blue", "red"), 
                       labels=c("True Function", "Sample from the posterior","Mean Value")) +#REPLICATES +
    geom_ribbon(data = data_gp, 
                aes(x, 
                    y = mean_curve, 
                    ymin = lower95, 
                    ymax = upper95,
                    fill="grey"), alpha = 0.6, show.legend = T) +
    scale_fill_manual("",values="gray", labels="95% CI") +
    geom_line(dat = data_gp, aes(x=x,y=mean_curve, color="red"), size=1) + #MEAN
    geom_point(data=obs_data,aes(x=x,y=y_norm),fill="green", color="yellow",shape=23, size=2) +
    geom_line(data=data_domain,aes(x=x_domain,y=y_domain, color="black"),size=1, alpha=0.7) +
    scale_y_continuous(lim=c(-0.5,1.2)) +
    scale_x_continuous(lim=c(0,1)) +
    theme(legend.position="none") +
    theme(legend.text=element_text(size=4)) +
    theme(text = element_text(size=6)) +
    theme(axis.title.x = element_blank(),
          axis.title.y = element_blank())
    #theme(legend.position="top")
  
}

##################################################################

utility_cal <- function(predict_list, x_predict,obs_data,eps) {
 
  y_max <- max(obs_data$y_norm)
  
  z <- (predict_list$mean - y_max - eps) / (predict_list$sd)
  
  utility <- (predict_list$mean - y_max - eps) * pnorm(z) + (predict_list$sd) * dnorm(z)
  
  new_x <- x_predict[which(utility==max(utility))] 
  
  return(new_x)
}

########################################################################


utility_cal_plot <- function(predict_list, x_predict,obs_data,eps,x_next) {
  
  y_max <- max(obs_data$y_norm)
  z <- (predict_list$mean - y_max - eps) / (predict_list$sd)
  
  utility <- (predict_list$mean - y_max - eps) * pnorm(z) + (predict_list$sd) * dnorm(z)
  
  data_utility <- data.frame(x=x_predict, utility=utility)
  
  ggplot(data_utility,aes(x,utility)) +
    geom_line() +
    scale_y_continuous(position = "right") +
    theme(text = element_text(size=6)) +
    geom_vline(xintercept = x_next, linetype="dotted", 
                color = "blue", size=0.5) +
   annotate("text", x=x_next, y=0, label= "x_next", hjust = -0.5, vjust=-2 ,colour = "blue") +
    theme(axis.title.x = element_blank(),
          axis.title.y = element_blank())
}

###############################################



plot_post_indi <- function(predict_list,x_predict,obs_data) {
  
  
  mv_sample <- mvtnorm::rmvnorm(100, predict_list$mean, predict_list$cov)
  ss <- t(mv_sample)
  
  dat <-data.frame(x=x_predict, ss) %>% 
    pivot_longer(-x, names_to = "rep", values_to = "value") %>% 
    mutate(rep=as.numeric(as.factor(rep)))
  
  data_gp <- data.frame(x=x_predict,upper95=predict_list$upper95,
                        lower95=predict_list$lower95, mean_curve=predict_list$mean)
  
  
  ggplot(dat,aes(x=x,y=value)) + 
    geom_line(aes(group=as.factor(rep), color="blue"), alpha=0.7) +
    #scale_colour_manual("",values = cols) +
    scale_color_manual("", values = c("black","blue", "red"), 
                       labels=c("True Function","Sample from the posterior","Mean Value")) +#REPLICATES +
    geom_ribbon(data = data_gp, 
                aes(x, 
                    y = mean_curve, 
                    ymin = lower95, 
                    ymax = upper95,
                    fill="grey"), alpha = 0.6, show.legend = T) +
    scale_fill_manual("",values="gray", labels="95% CI") +
    geom_line(dat = data_gp, aes(x=x,y=mean_curve, color="red"), size=1) + #MEAN
    geom_point(data=obs_data,aes(x=x,y=y_norm),fill="green", color="yellow",shape=23, size=2) +
    geom_line(data=data_domain,aes(x=x_domain,y=y_domain, color="black"),size=1, alpha=0.7) +
    scale_y_continuous(lim=c(-0.5,1.25)) +
    scale_x_continuous(lim=c(0,1)) +
    xlab("x") +
    ylab("f(x)") +
    theme(legend.position="top") 
  
}

utility_cal_plot_ind <- function(predict_list, x_predict,obs_data,eps,x_next) {
  
  y_max <- max(obs_data$y_norm)
  z <- (predict_list$mean - y_max - eps) / (predict_list$sd)
  
  utility <- (predict_list$mean - y_max - eps) * pnorm(z) + (predict_list$sd) * dnorm(z)
  
  data_utility <- data.frame(x=x_predict, utility=utility)
  
  ggplot(data_utility,aes(x,utility)) +
    geom_line() +
    geom_vline(xintercept = x_next, linetype="dotted", 
                color = "blue", size=1) +
   annotate("text", x=x_next, y=0, label= "x_next", hjust = 1.2, vjust=-1 ,colour = "blue") 
   
}

```

```{r exampleshow, fig.retina=2, warning=FALSE, fig.cap="Ite1 - Top: Gaussian posterior over the initial sample points; Lower: Utility function over the x values", out.width="100%", fig.align='left', echo=FALSE, out.height="75%"}

set.seed(123)
x <- c(0.05,0.2,0.5,0.6,0.95)
obs_data <- obs_data_return(x)
x_predict <- seq(0,1,0.005)

predict_list <- km_model(obs_data,x_predict)
posterior_1 <- plot_post_indi(predict_list$predict_list,x_predict,obs_data)

new_x_point1 <- utility_cal(predict_list$predict_list,x_predict,obs_data,0.1)
utility_1 <- utility_cal_plot_ind(predict_list$predict_list,x_predict,obs_data,0.1, new_x_point1)

posterior_1 /
  utility_1
```

Calling the Figure \@ref(fig:exampleshow) as the iteration \# 1, now we can start sampling sequentially. In the Figure \@ref(fig:allinone) another two iteration has been provided. Wher in each row, the plot on the left represents the posterior on the gaussian condistioning, the right show the utility function. Note that in the Figure \@ref(fig:allinone) all axis labels and legned were not included, to have better visibity. (more info about each plot can ben found in \@ref(fig:exampleshow)) . Interesting to see tat in this example case, at the irteration \#2, the workflow query the point $x=0.385$ which presents the best point so far found through BayesOpt workflow. Thefiore, after just two iteration we are around $\frac{x_{best}}{x_{M}}=\frac{0.385}{0.390}=98.7%$ of the global optima. Although this is case for 1-D problem, it is clearly showing the stength of the workflow to approach the lglobal optima, in as few as possible iteration. In this case after iteration\#2, tyhe total number of tim,e that the real function has been quered is $\text{size}(\mathcal{D}) + \text{size}(total iteration) = 5 + 2=7$ .

```{r allinone, echo=FALSE, fig.align='left', fig.cap="Gaussian posterior of over the initial sample points", fig.retina=2, message=FALSE, warning=FALSE, out.height="90%", out.width="90%"}
set.seed(123)

x <- c(0.05,0.15,0.2,0.5,0.6,0.95)
obs_data <- obs_data_return(x)
x_predict <- seq(0,1,0.005)

predict_list <- km_model(obs_data,x_predict)
posterior_1 <- plot_post(predict_list$predict_list,x_predict,obs_data)

new_x_point1 <- utility_cal(predict_list$predict_list,x_predict,obs_data,0.1)
utility_1 <- utility_cal_plot(predict_list$predict_list,x_predict,obs_data,0.1, new_x_point1)
#utility_1
#################

x2 <- c(0.05,0.15,0.2,0.5,0.6,0.95,new_x_point1)
obs_data2 <- obs_data_return(x2)
x_predict <- seq(0,1,0.005)

predict_list2 <- km_model(obs_data2,x_predict)
posterior_2 <- plot_post(predict_list2$predict_list,x_predict,obs_data2)
new_x_point2 <- utility_cal(predict_list2$predict_list,x_predict,obs_data2,0.1)
utility_2 <- utility_cal_plot(predict_list2$predict_list,x_predict,obs_data2,0.1, new_x_point2)

###################

x3 <- c(0.05,0.15,0.2,0.5,0.6,0.95,new_x_point1,new_x_point2)
obs_data3 <- obs_data_return(x3)
x_predict <- seq(0,1,0.005)

predict_list3 <- km_model(obs_data3,x_predict)
posterior_3 <- plot_post(predict_list3$predict_list,x_predict,obs_data3)

new_x_point3 <- utility_cal(predict_list3$predict_list,x_predict,obs_data3,0.1)
utility_3 <- utility_cal_plot(predict_list3$predict_list,x_predict,obs_data3,0.1, new_x_point3)

##################

# (posterior_1 + utility_1) / 
# (posterior_2 + utility_2) / 
# (posterior_3 + utility_3) 
  #plot_layout(ncol = 2)


library(gridExtra)
grid.arrange(posterior_1, utility_1, posterior_2, utility_2, posterior_3, utility_3, ncol=2,
             widths = c(6, 4))  
# (posterior_3 + utility_3) , ncol=2)

# (posterior_1 + theme(plot.margin = unit(c(0,30,0,0), "pt"))) +
# (utility_1 + theme(plot.margin = unit(c(0,0,0,30), "pt")))
# p1 + p2 + p3 + p4 + 
#   plot_layout(ncol = 3)
```

Before going to apply the same workflow at the field scale, the 1-D example presented here offer another useful feature of the Bayesian Optimisation. Looking at \@ref(fig:allinone), we can see that the maximum of utility function is at the iteration \# 3 in order of $10^{-6}$ . That show that after optimization, eve best point to be queried in the next section has a very little utility. So can safely stop the process, since querying points to be sampled from the expensive function has a negligible potential to improve our search in optimization.

\newpage

## Field Scale

### Synthetic 3D Reservoir Model

In this section, the BayesOpt workflow is applied to the synthetic 3D reservoir model. The trough introduction of the model and gelogical describtion can be found in [@jansen2014] . Known as "Egg Model" it has a geology of channelized depositional system.

```{r eggbase, echo=FALSE, fig.align='center', out.width="300px", fig.retina=2, fig.cap="Well locations in Egg model, blue ones are injection, the red producers"}
knitr::include_graphics("img/egg_base.jpg")
```

The 3D model has eight water injectors and four producers wells shown in Figure \@ref(fig:eggbase). The has a geological realizations of patterns of highly permeable channels which are described by 100 equi-probable geological realizations, three of which are illustrated in left side of Figure \@ref(fig:combine).[@hong2017cg].

Relative permeabilities and the associated fractional flow curve of the model have shown in right side of Figure \@ref(fig:combine) .All the wells are vertical and completed in all seven layers. Capillary pressure is ignored. The reservoir rock is assumed to be incompressible. The model has a life-cycle of 10 years. Here, the injection rate to be maiintaned over life-cycle of reservoir is going to be optimized. Thus, given eight injection wellls, the optimizatijon workflow has the eight dimnetions.However, the optimization in not unbounded, the water can be adjusted from 0 to 100 m3/day, making the box-constrain optimization. The injectors are operated with no pressure constraint, and the producers are under a minimal BHP of 395 bars without rate constraint.

```{r combine, echo=FALSE,out.width="100%",fig.show='hold', fig.align='center', fig.cap="Left: Three geological realizations of the 3D model; Right: Rel perm and fractional flow curve"}
knitr::include_graphics(c("img/combine.jpg"))
```

### Well Control Optimization

Reviewing the equation raised in the section 3, here the goal is robust optimization of the field , given geological realizations as follow:

```{=tex}
\begin{equation}
\text{Objective Func(u)}= \overline{J}(u) = \frac{\sum_{i=1}^{n_e} J_r(u,G_i)}{n_e}  (\#eq:npvoptrep)
\end{equation}
```
Equation \@ref(eq:npvoptrep)

$u$ is Injection rate for the each injection well, therefore the control vector, to be optimizaed in this case is defined as:

```{=tex}
\begin{equation}
u=[u_{inj1},u_{inj2},u_{inj3},u_{inj4},u_{inj5},u_{inj6},u_{inj7},u_{inj8}]^{\intercal} 
\label{eq:cont-vec}
\end{equation}
```
As the \@ref(eq:npvoptrep) suggest, the $\overline{J}(u)$ need some parameters to be defined. The oil price ($P_o$), water production cost ($p_{wp}$) and water injection cost ($P_{wi}$) in $dollar/m^3$ has been provided in the Table \@ref(tab:npvparam). Also, in this work the cash flow is disconted daily and the discount factor is avilable in the \@ref(tab:npvparam). We would like to note that in this work due to avoid further computional burden in optimization process, 10 realizations of the egg model has been considered, therefore $n_e=10$ in Equation \@ref(eq:npvoptrep).

```{r npvparam, echo=FALSE}

price_data <- tibble(item_1=c("P_o","P_wp","P_wi"),
                     price=c("315","47.5","12.5"),
                     item_2=c("b","D","n_e"),
                     price2=c("8%","365","10"))

colnames(price_data) <- c("Item","Pric", "Items", "Value")
#price_data %>%  kableExtra::kable(format = "html", escape = FALSE) %>%
#    kable_styling()
kbl(price_data, format = "latex", caption = "Required Parameters needed for calculation of Expected NPV")%>%
  kable_styling(position = "center", latex_options = "HOLD_position", full_width = T)
```

### BayesOpt Workflow

As it was discussed, the starting point of the BAyesOpt workflow is to randomly sample the initial data pairs $\mathcal{D}$ which is used to build the Gaussian model of the response surface to the input variables. In this work, forty samples fom the Latin hyper cube sampling (LHS) method were drawn. The LHS is prefred in this work to Monte Carlo since it provides the stratifcation of the CDF of each variable, leading to better coverage of the input variable space. The Figure \@ref(fig:lhssampling) show the results of the $\overline{J}(u)$ for each sample from LHS. Also, The maximum $\overline{J}(u)$ found from sampling has been shown with blue line. Setting the specific seed number (since LHS is in itself is random process), we get the max $NPV$ aciehved here was $35.65 \$MM$. Looking at Figure \@ref(fig:lhssampling) it is worth to mention that random sampling like the LHS is not helpful to consistently approach the global optimum point, and there is a need for efficient workflow to find the optimum point while using the a few as possible sampling from real function.

```{r lhssampling, echo=FALSE, fig.retina=2, fig.align='center', fig.cap="Expected NPV as result of forty sampling from LHS"}
bo_123$scoreSummary[1:40,] %>% 
  arrange(Score) %>% 
  ggplot(aes(Iteration, Score)) +
  geom_point(colour = "blue", size = 3)+
  xlab("Sample Number") + ylab("Expected NPV (in $MM)") +
  scale_x_continuous(limits = c(1, 40),breaks = seq(0,40,2))+
  scale_y_continuous(limits = c(28, 38),breaks = seq(28,38)) +
  geom_hline(aes(yintercept = max(bo_123$scoreSummary[1:40,]$Score))) +
  geom_text(aes(5, max(bo_123$scoreSummary[1:40,]$Score), 
                label = "Maximum Point in LHS", vjust = - 1, fontface="italic"), fontface="italic", size=4, data = data.frame()) +
  labs(x = TeX("Number of Samples, $# \\bar{J}(u)$"))
```

Having the initial data found through LHS, we can build the probalistic model of the reposnse surface and sequentially sample from the *expensive-to-evaluate* function. Unfortunately, win this section we can not plot the posterior of the probalistic model, condition on the above forty LHS samples, due being the space is eight-dimetional, and hard to visulize. The Figure \@ref(fig:lhsbayesop) shows the expected NPV found after ten sequential sampling resulted from the BayesOpt workflow. Readers are refreed to this point that in the figure, not all red points are increasing and some points are lower than previous points. The reason for this behaviour is the nature of BayesOpt algorith. We can suggest that in the points that has lower expected NPV from the previous, we may reached the lower optimum point, but those points helped us to decrease the uncertainity, which is helpful for the further sampling. We can see that after just ten evaluation of the expenside function (here it means finding the expected NPv from running 10 geological realization using flow simulation) we reach the new record Expeted NPV of $max \overline{J}(u)=36.85$$\$MM$.

```{r lhsbayesop, echo=FALSE, fig.retina=2, fig.align='center', fig.cap="Blue points represnts the sample from LHS, red points represents the samples from the BayesOpt Workflow"}
opt_type <- c(rep("LHS Sampling",40),rep("Bayesian Opt",10))

bo_123$scoreSummary[1:50,] %>% 
  add_column(Sampling_Scheme = opt_type) %>% 
  ggplot(aes(Iteration, Score, colour = Sampling_Scheme)) +
  geom_point(size = 3)+
  labs(x = TeX("Number of Samples, $# \\bar{J}(u)$")) +
  ylab("Expected NPV (in $MM)") +
  scale_x_continuous(limits = c(1, 50),breaks = seq(0,50,5))+
  scale_y_continuous(limits = c(28, 37),breaks = seq(28,37)) +
  scale_color_manual(values = c("red", "blue")) +
  geom_hline(aes(yintercept = max(bo_123$scoreSummary[1:40,]$Score)), alpha=0.4, 
             linetype = "dashed") +
  annotate("text", x=10, y=max(bo_123$scoreSummary[1:40,]$Score), 
           label=("Maximum Point in LHS"), size=4, color="blue", vjust = - 1, fontface="italic") +
  geom_hline(aes(yintercept = max(bo_123$scoreSummary[1:50,]$Score)), alpha=0.4, 
             linetype = "dashed") +
  annotate("text", x=30, y=max(bo_123$scoreSummary[1:50,]$Score), 
           label=("Maximum Point in BayesOpt"), size=4, color="red", vjust = - 1, fontface="italic") +
  theme(legend.position = "none")
```

Now, as we explained in the 1-D section, the plot of the utility at each iteration could provide some useful information about the optimization process. The Figure \@ref(fig:utilitycurve) plots the $\alpha_{EI}^*(\mathcal{D}, \theta^*)$ (Equation \@ref(eq:exp-easy) )versus the ten iteration in this work. In fact the notaion $\alpha_{EI}^*$ means the optimum of the $\alpha_{EI}(u;\mathcal{D},\theta^*)$ after running multi-start (1000)- L-BFGS-B on all $u$ values. Now, we can see that in the figure the $\alpha_{EI}^*$ is decreasing going toward the zero. It can be inferred from this trend that, we are going out of the *good* $u$ values to be sampled from the expensive function, can be intepreted that we are in the vicinity of global optima, if we see after several iteration still $\alpha_{EI}^*$ is less than $10^-6$.

```{r utilitycurve, echo=FALSE,fig.retina=2, fig.align='center', fig.cap="Maximum utility at each iteration, after running L-BFGS-B to find the u with max utility, $\\alpha_{EI}^*$"}
bo_123$scoreSummary %>% 
  filter(acqOptimum == TRUE) %>% 
  ggplot(aes(Epoch, gpUtility)) +
  geom_point(size=3, color="yellow", shape=23, fill="firebrick") +
  geom_line(linetype="dashed") +
  scale_x_continuous(limits = c(1, 10),breaks = seq(1,10,1)) +
  xlab("Iteration") +
  labs(y = TeX("Maximum Utility at each iteration, $\\alpha_{EI}^*(D,\\theta^*)$"))
```

Given that the BayesOpt inherintely has stochasric natrae ( from this perspective that having thje diffrenet initialization in LHS sampling will affect the final solution), in this section BayesOpt is repeated with diffret initilization. Ideally, this repeation shouwl be conducted 100 or 1000 times, to get better overview of the convergence of the algorithm given diffrent initilization. Though, because of the computional burden, in this work only three repeations were performed. optimization Repeat the Optimization, three times, in different initial design points. Figure \@ref(fig:difinit) shows results of three repeations. At each repeation (top, middle, bottom), the blue dots come from diffrente seed numbers and they are diffrente. Then, gicen that initialization $\mathcal{D}$, sequential sampling from the expenive function is perfomred, shown in the red points. Like previous case, in these repeations, 40 samples drawn from LHS algortihem, the 10 were taken thorigh BAyesOpt lagorith, totaling 50 samples. At each row of the Figure \@ref(fig:difinit), two horizontal lines show the maximum point $\overline{NPV}$ in both random sampling phase (LHS) and BayesOpt phase. As it can be noted from the Figure \@ref(fig:difinit), at each repeation, the BayesOpt will improve the solution with small sample evaluation of the $\overline{J}(u)$. Thefore, improvemnet following the BayesOpt phase indepned of the initial design, yet the bigger question is whether given different initial design, the algorithm converge the vicinity of global optima. What is refered here is that if having different initilaization will lead completely different final solution, that hints that the algorithm has a "local" search, in conrast, if the solutions leads to one specif close $u^*$, that reprsents that algorithm have a "global" view on the surface of the objective function. In the case of "global" optimization having diferent initilizatin should lead to simular final solution, since the algorithm will not get stuck in local optimum points, close to initilalized data. This is common practice in the gradient-based optimization where the algorithm is powerfull in local optimization and in order to avoid stuck in local exterme points, "multi-start" runs are performed in order to search the global point in the objective function.

```{r difinit,fig.retina=2, echo=FALSE,fig.align='center', out.height="90%", out.width="90%", fig.cap="BayesOpt workflow applied to Syntetic 3D model, in three different initialization", message=FALSE, error=FALSE, warning=FALSE}

opt_type <- c(rep("LHS Sampling",40),rep("Bayesian Opt",10))

plot_bo_123 <- bo_123$scoreSummary[1:50,] %>% 
  add_column(Sampling_Scheme = opt_type) %>% 
  ggplot(aes(Iteration, Score, colour = Sampling_Scheme)) +
  geom_point(size = 2)+
  labs(x = TeX("Number of Samples, $# \\bar{J}(u)$")) + ylab("Expected NPV (in $MM)") +
  scale_x_continuous(limits = c(1, 50),breaks = seq(0,50,5))+
  scale_y_continuous(limits = c(28, 38),breaks = seq(28,38)) +
  scale_color_manual(values = c("red", "blue")) +
  geom_hline(aes(yintercept = max(bo_123$scoreSummary[1:40,]$Score)), alpha=0.4, linetype = "dashed") +
  annotate("text", x=15, y=max(bo_123$scoreSummary[1:40,]$Score), 
           label=("Maximum Point in LHS"), size=2, color="blue", vjust = - 1, fontface="italic") +
  geom_hline(aes(yintercept = max(bo_123$scoreSummary[1:50,]$Score)), alpha=0.4, linetype = "dashed") +
  
  annotate("text", x=30, y=max(bo_123$scoreSummary[1:50,]$Score), 
           label=("Maximum Point in BayesOpt"), size=2, color="red", vjust = - 1, fontface="italic") +
  
  theme(legend.position = "none") +
  theme(axis.text = element_text(size = 6)) +
  theme(axis.title = element_text(size = 6))


plot_bo_1234 <- bo_1234$scoreSummary[1:50,] %>% 
  add_column(Sampling_Scheme = opt_type) %>% 
  ggplot(aes(Iteration, Score, colour = Sampling_Scheme)) +
  geom_point(size = 2)+
  labs(x = TeX("Number of Samples, $# \\bar{J}(u)$")) + ylab("Expected NPV (in $MM)") +
  scale_x_continuous(limits = c(1, 50),breaks = seq(0,50,5))+
  scale_y_continuous(limits = c(28, 38),breaks = seq(28,38)) +
  scale_color_manual(values = c("red", "blue")) +
  geom_hline(aes(yintercept = max(bo_1234$scoreSummary[1:40,]$Score)), alpha=0.4, linetype = "dashed") +
  
  annotate("text", x=15, y=max(bo_1234$scoreSummary[1:40,]$Score), 
           label=("Maximum Point in LHS"), size=2, color="blue", vjust = - 1, fontface="italic") +
  
  geom_hline(aes(yintercept = max(bo_1234$scoreSummary[1:50,]$Score)), alpha=0.4, linetype = "dashed") + 
  annotate("text", x=30, y=max(bo_1234$scoreSummary[1:50,]$Score), 
           label=("Maximum Point in BayesOpt"), size=2, color="red", vjust = - 1, fontface="italic") +  
  
  theme(legend.position = "none") + 
  theme(axis.text = element_text(size = 6)) +
  theme(axis.title = element_text(size = 6)) # change axis titles


plot_bo_12345 <- bo_12345$scoreSummary[1:50,] %>% 
  add_column(Sampling_Scheme = opt_type) %>% 
  ggplot(aes(Iteration, Score, colour = Sampling_Scheme)) +
  geom_point(size = 2)+
  labs(x = TeX("Number of Samples, $# \\bar{J}(u)$")) + ylab("Expected NPV (in $MM)") +
  scale_x_continuous(limits = c(1, 50),breaks = seq(0,50,5))+
  scale_y_continuous(limits = c(28, 38),breaks = seq(28,38)) +
  scale_color_manual(values = c("red", "blue")) +
  geom_hline(aes(yintercept = max(bo_12345$scoreSummary[1:40,]$Score)), alpha=0.4, 
             linetype = "dashed") +
  
  annotate("text", x=15, y=max(bo_12345$scoreSummary[1:40,]$Score), 
           label=("Maximum Point in LHS"), size=2, color="blue", vjust = - 1, fontface="italic") +
  geom_hline(aes(yintercept = max(bo_12345$scoreSummary[1:50,]$Score)), alpha=0.4, 
             linetype = "dashed") +
  annotate("text", x=30, y=max(bo_12345$scoreSummary[1:50,]$Score), 
           label=("Maximum Point in BayesOpt"), size=2, color="red", vjust = - 1, fontface="italic") +  
  theme(legend.position = "none") +
  theme(axis.text = element_text(size = 6)) +
  theme(axis.title = element_text(size = 6))

grid.arrange(plot_bo_123, plot_bo_12345, plot_bo_1234, ncol=1)

```

To further continue thiss dicussion on the effect of initialization on the final solution, the $u^*$ value for each repeatation has been show on the left side of Figure \@ref(fig:diffu). Where the $u^*$ is the vector of 8 dimention, each value shows the optimum injection rate for the 10 years life cycle of the field, in $m^3/D$. We woul like to note that the y axis was plotted from the range of 5 to 100. The reason for this is to show that in this optimization problem, injection of each wells can take any number between 5$m^3/D$ to 100 $m^3/D$, and the y axis shows the full extend of the value optimum zation worlkflow can reach. Visually, looking at the left plot at Figure \@ref(fig:diffu), we can see that the final solution of three repeations at each weels, does not differ significantly from each other . With small exception of (injection \#2), it seems all the final solutions converges to the same solution. This feature that can be loosly said as "robustness" of optimization workflow to initial design is very helpfu, from this sense that we do not neeed to resetart the optimization with different initilaization, since they all will converges to the similar solution. From this perspective, authours can infere that BayesOpt workflow can be considered as "global" optimization method, as it shows the workflow avoids stuck in local exterme pointsor saddle regions. The plot on the left side of Figure \@ref(fig:diffu) shows that mean injection rate (mean of three repeations) and erro bar at each injection wells. The bottom of error bar in this plot shows the $mean-sd$ and top of bar is $mean + sd$ . As we can see that we do not see significant variation in the final solution in each repeations, also the plots recoomnds that in the case of repeating the optimization with more than three times (like 10 or 100), it can lead to lower variation in final solution.

```{r diffu, echo=FALSE, fig.retina=2, fig.align='center', out.width="90%", fig.cap="Left: final solution of optimization algorithm in three different initialization, Right: Mean and error bar of each injection rate at each injection wells"}

##############
library(ParBayesianOptimization)
u_best_123 <- getBestPars(bo_123)
u_best_1234 <- getBestPars(bo_1234)
u_best_12345 <- getBestPars(bo_12345)

###########

list_best_u <- list(u_best_123,u_best_1234,u_best_12345)
df_list_best_u <- data.frame(matrix(unlist(list_best_u), byrow = F, nrow = 8))
df_list_best_u$inj <- c("Inj1","Inj2","Inj3","Inj4","Inj5","Inj6","Inj7","Inj8")
df_list_best_longer <- df_list_best_u %>% 
  pivot_longer(-inj,names_to = "Replication", values_to = "Injection_Rate")


###########

p1 <- ggplot(data=df_list_best_longer, aes(x=inj, y=Injection_Rate, fill=Replication)) +
geom_bar(width = 0.4, stat="identity", position=position_dodge()) +
  coord_cartesian(ylim = c(5, 100)) +
  scale_color_manual(labels = c("Random LHS #1", "Random LHS #2", "Random LHS #3"),
                     values = c("red", "blue", "green"),
                     aesthetics = "fill") +
  theme(legend.position = "top") +
  theme(legend.title = element_text(colour="black", size=10, 
                                      face="bold"))


df_new <- df_list_best_longer %>% 
  group_by(inj) %>%
  mutate(upper = mean(Injection_Rate) + sd(Injection_Rate), 
         lower = mean(Injection_Rate) - sd(Injection_Rate))


p2 <- ggplot(df_new, aes(x = inj, y = Injection_Rate)) +
  stat_summary(fun = mean, geom = "bar", position = position_dodge(width = .9),size = 3) + 
  geom_errorbar(aes(ymin = lower, ymax = upper),
                width = .2,                    # Width of the error bars
                position = position_dodge(.9),
                color='#E69F00') +
  coord_cartesian(ylim = c(5, 100))

p1+p2
```


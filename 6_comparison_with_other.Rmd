\newpage

# Comparison with other Optimization Alternatives

In this section, the aim is to compare the performance of the BO workflow with other available optimization algorithms commonly used for production optimization under uncertainty. The literature of production optimization enjoys a wide variety of workflows and algorithms. Broadly speaking, those can be divided into two categories gradient-based and gradient-free. Gradient-based methods, such as those described in [@forouzanfar2014; @li2012; @volkov2018] can provide a computational advantage in terms of efficiency. They are, however, local methods, and it is known that broad (global) searches can be advantageous in well-control optimization[@debrito2021]. Therefore, in this work, two well-known gradient-free optimization methods^[Also, we note that we decided to compare BO with gradient-free methods since BO belongs to that category. In essence, we can say that BO is a gradient-free method (as it does not need any gradient calculation), and we found it appropriate to compare BO with GA and PSO.], extensively used for production optimization, named Genetic Algorithm (GA) [@chai2021; @holland1992] and Particle Swarm Optimization (PSO), [@eberhart1995; @jesmani2016] have been considered. This section provides a brief overview of each method, but interested readers are referred to the original papers.[@eberhart1995; @holland1992]

## Particle Swarm Optimization (PSO)

PSO is a global stochastic search technique that operates based on analogies to the behaviors of swarms/flocks of living organisms. Initially developed by [@eberhart1995], considering a swarm with $P$ particles, there is a position vector $\mathbf{u}_{i}^{t}=(\mathbf{u}_{i1},\mathbf{u}_{i2}, \mathbf{u}_{i3},\mathbf{u}_{in})^T$ and a velocity vector $V^t_i=(v_{i1},v_{i2},v_{i3},v_{in})^T$ at an $t$ iteration for each one of the $i$ particle that composes it.^[Here, when we write ($\mathbf{u}_{i}^{t}=(\mathbf{u}_{i1},\mathbf{u}_{i2}, \mathbf{u}_{i3},\mathbf{u}_{in})^T$, it means that $\mathbf{u}_{i}^{t}$ is a control vector with dimension of $n$.] These vectors are updated through the dimension $j$ according to the following equations:

```{=tex}
\begin{equation}
V^{t+1}_{ij} = \omega V^{t}_{ij} + c_{1}r_{1}^{t}(pbest_{ij}-\mathbf{u}_{ij}^t) + c_2r_2^t(gbest_j-\mathbf{u}_{ij}^{t})
\label{eq:pso}
\end{equation}
```

where $i=1,2,..., P$ and $j =1,2,...,n$. Equation \@ref(eq:pso) explains three different contributions to a particle's movement in an iteration. In the first term, the parameter $\omega$ is the inertia weight constant. In the second term, the parameter $c_1$ is a positive constant and it is an individual-cognition parameter, and it weighs the importance of the particle's own previous experiences. The other parameter in the second term is $r_1^t$, is a random value parameter with [0,1] range. The third term is the social learning one. Because of it, all particles in the swarm are able to share the information of the best point achieved regardless of which particle had found it, for example, $gbestj$. Its format is just like the second term, the one regarding individual learning. Thus, the difference $(gbest_j - \mathbf{u}^t_{ij})$ acts as an attraction for the particles to the best point until found at some t iteration. Similarly, $c_2$ is a social learning parameter, and it weighs the importance of the global learning of the swarm. $r_2$ plays exactly the same role as $r_1$. The control vector (particle's position) $\mathbf{u}_{i}^{t}$ then is updated in each iteration as: [@almeida2019]

```{=tex}
\begin{equation}
\mathbf{u}_{ij}^{t+1} = \mathbf{u}_{ij}^{t} + V_{ij}^{t+1}
\label{eq:psoup}
\end{equation}
```

## Genetic Algorithm (GA)

Genetic algorithm is another optimization algorithm that uses evolutionary strategies inspired by the basic principles of biological evolution. First developed by John Holland [@holland1975] and his collaborators in the 1960s and 1970s, later applied for optimization and search problems @goldberg1988; @mitchell1998. The evolution process is as follows: GA starts with generating an initial random population of size $P$, so for step $k = 0$ we may write ${\mathbf{u}_1^{(0)}; \mathbf{u}_2^{(0)},\cdots, \mathbf{u}_p^{(0)}}$, (step 1). The fitness of each member of the population at any step $k$, $\mathbf{\overline{J}}(\mathbf{u}_i^{(k)})$, is computed, and probabilities $p_i^{(k)}$ are assigned to each individual in the population, usually proportional to their fitness, (step 2). The reproducing population is formed (known as **selection**) by drawing with replacement a sample where each individual has a probability of surviving equal to $p_i^{(k)}$, (step 3). A new population ${\mathbf{u}_1^{(k+1)}; \mathbf{u}_2^{(k+1)},\cdots, \mathbf{u}_p^{(k+1)}}$ is formed from the reproducing population using crossover and mutation operators, step (4). Then, set $k = k + 1$ and the algorithm returns to the fitness evaluation step, (back to step 2). When convergence criteria are met, evolution stops, and the algorithm delivers as the optimum [@scrucca2013].

```{r, echo=FALSE, results='hide'}
bo_12345_npv_result <- bo_12345$scoreSummary$Score
bo_12345_npv_result[1:40] <- max(bo_12345_npv_result[1:40])
bo_12345_npv_ite <- bo_12345$scoreSummary$Iteration

for (i in seq(41,50)) {
  
  if (bo_12345_npv_result[i] < bo_12345_npv_result[i-1]){
    
    bo_12345_npv_result[i] <- bo_12345_npv_result[i-1] 
    
  }
  
}

bo_1234_npv_result <- bo_1234$scoreSummary$Score
bo_1234_npv_result[1:40] <- max(bo_1234_npv_result[1:40])
bo_1234_npv_ite <- bo_1234$scoreSummary$Iteration

for (i in seq(41,50)) {
  
  if (bo_1234_npv_result[i] < bo_1234_npv_result[i-1]){
    
    bo_1234_npv_result[i] <- bo_1234_npv_result[i-1] 
    
  }
  
}

bo_123_npv_result <- bo_123$scoreSummary$Score
bo_123_npv_result[1:40] <- max(bo_123_npv_result[1:40])
bo_123_npv_ite <- bo_123$scoreSummary$Iteration

for (i in seq(41,50)) {
  
  if (bo_123_npv_result[i] < bo_123_npv_result[i-1]){
    
    bo_123_npv_result[i] <- bo_123_npv_result[i-1] 
    
  }
  
}

#######################BO_50 ############################
ss_50 <- bo_123_50$scoreSummary
#ss <- bo_123$scoreSummary

#plot(bo_123_50)
#getBestPars(bo_123_50)

########### GA ######################################

ga_12345_npv_result <- rep(ga_12345@summary[,"max"],each=25)
ga_12345_npv_ite <- seq(1,250)

ga_1234_npv_result <- rep(ga_1234@summary[,"max"],each=25)
ga_1234_npv_ite <- seq(1,250)

ga_123_npv_result <- rep(ga_123@summary[,"max"],each=25)
ga_123_npv_ite <- seq(1,250)

########### PSO #######################################
#
pso_12345_npv_result_i <- rep(0,10)
for (i in 1:10) {
  pso_12345_npv_result_i[i] <- max(-pso_12345$stats$f[[i]])
}

pso_12345_npv_result_i
pso_12345_npv_result <- rep(pso_12345_npv_result_i,each=25)
pso_12345_npv_ite <- seq(1,250)

#
pso_1234_npv_result_i <- rep(0,10)
for (i in 1:10) {
  pso_1234_npv_result_i[i] <- max(-pso_1234$stats$f[[i]])
}
pso_1234_npv_result <- rep(pso_1234_npv_result_i,each=25)
pso_1234_npv_ite <- seq(1,250)

#
pso_123_npv_result_i <- rep(0,10)
for (i in 1:10) {
  pso_123_npv_result_i[i] <- max(-pso_123$stats$f[[i]])
}
pso_123_npv_result <- rep(pso_123_npv_result_i,each=25)
pso_123_npv_ite <- seq(1,250)

#pso_123$par
#####################################################################

NPV_max_data <- c(bo_12345_npv_result, bo_1234_npv_result, bo_123_npv_result, 
                  ga_12345_npv_result, ga_1234_npv_result, ga_123_npv_result,
                  pso_12345_npv_result, pso_1234_npv_result, pso_123_npv_result)


Reservoir_Simulation_number = c(bo_12345_npv_ite, bo_1234_npv_ite,bo_123_npv_ite, 
                                ga_12345_npv_ite, ga_1234_npv_ite, ga_123_npv_ite,
                                pso_12345_npv_ite, pso_1234_npv_ite,pso_123_npv_ite)


methods_alg <- c(rep("BO",50), rep("BO",50),rep("BO",50),
                 rep("GA",250), rep("GA",250),rep("GA",250),
                 rep("PSO",250), rep("PSO",250),rep("PSO",250))

seed_alg <- c(rep("Repeat1",50), rep("Repeat2",50),rep("Repeat3",50),
                 rep("Repeat1",250), rep("Repeat2",250),rep("Repeat3",250),
                 rep("Repeat1",250), rep("Repeat2",250),rep("Repeat3",250))

comp_data_frame <- tibble(NPV_max= NPV_max_data, 
                          Reservoir_Simulation = Reservoir_Simulation_number,
                          method=methods_alg,
                          see_number=seed_alg) 

#ggplot(comp_data_frame, aes(Reservoir_Simulation, NPV_max, colour=method)) +
#  geom_point() +
#  facet_grid(cols = vars(see_number)) +
#  xlab("Required Number of Reservoir Simulation (forward modeling)") +
#  ylab("-Max NPV Reached")

```

## Comparison with Fixed Number of Running Reservoir Simulator (N=50)

In the first part of the comparison, we compare the BO with PSO and GA in a fixed number of $\overline{\mathbf{J}}(\mathbf{u})$ evaluations. It means that the optimization process could continue, until they use $\overline{\mathbf{J}}(\mathbf{u})=50$ function evaluations. In fact $\overline{\mathbf{J}}(\mathbf{u})=50$ is equal to $500$ reservoir simulations, due to the number of realization, $n_e=10$. These two methods need parameters to be defined by the user. In GA, these parameters are: population Size, probability of crossover between pairs of chromosomes, probability of mutation in a parent chromosome, the number of best fitness individuals to survive at each generation. For PSO, the algorithm parameters are the swarm's size, the local exploration constant, and the global exploration constant.

In Figure \@ref(fig:comp-fixbud) results of comparison has shown. Since all three algorithms are stochastic (meaning they depend on initial random samples), the comparison has been repeated three times. We want to note that in Figure \@ref(fig:comp-fixbud) the $y$ axis is "Max E(NPV) Reached", meaning that in each generation of GA and PSO algorithm, "Max" of the each generation has been shown. Moreover, the Figure shows that in BO method, number of $\overline{\mathbf{J}}(\mathbf{u})$ grows as $n_{initial} + n_{iteration}$, which in this work $n_{initial}=40$ and $n_{iteration}=10$, summing up to $50$. Whereas, in PSO and GA, number of $\overline{\mathbf{J}}(\mathbf{u})$ grows as $n_{\text{popsize}}\times iteation$. As Figure \@ref(fig:comp-fixbud) shows, the BO outperforms the other two algorithms in all repetition by reaching a higher expected NPV at a fixed computational resource. Part of performance could be attributed how algorithms use the forward model. In BO, after initial sampling, the algorithm sequentially queries a "one" from the expensive function, while GA and PSO need another sample size $n_p$ for each iteration.

```{r, message=FALSE, error=FALSE, fig.align='center'}
ga_param <- data.frame(parameters = c("Population Size", "Probability of crossover", "Probability of mutation", "Number of best fitness individuals to survive"), value = c(25, "80%", "20%", "5%"))

pso_param <- data.frame(parameters = c("Size of the swarm", "Local exploration constant", "Global exploration constant"), value = c(25, "5+log(2)", "5+log(2)"))

param_table <- rbind(pso_param, ga_param)
  
kbl(param_table, booktabs = T, caption = "Parameters of GA and PSO Methods")%>%
  #kable_paper("striped", full_width = F) %>%
  pack_rows("PSO", 1, 3) %>%
  pack_rows("GA", 4, 7) %>% 
  kable_styling(position = "center", full_width = T)

```

## Comparison with Number of Running Reservoir Simulator (N=50) for BO, (N=250) for PSO and GA


```{r comp-fixbud,echo=FALSE, fig.retina=2, fig.align='center', out.width="90%", fig.cap="Comparison of GA, PSO and BayesOpt performance at fixed function evaluation budget"}
comp_data_frame_50sample <- comp_data_frame %>% 
  group_by(method) %>% 
  filter(Reservoir_Simulation<51 & Reservoir_Simulation>0)

ggplot(comp_data_frame_50sample, aes(Reservoir_Simulation, NPV_max, group=method)) +
  geom_point(aes(shape=method, color=method), size=2)+
  scale_shape_manual(values=c(16, 5, 0))+
  scale_color_manual(values=c('red','#E69F00', '#56B4E9'))+
  theme(legend.position="top") +
  facet_grid(cols = vars(see_number)) +
  labs(x = TeX("Number of Required, $ \\bar{J}(u)$ Evaluation")) +
  #xlab("Required #of Reservoir Simulation (forward modeling)") +
  ylab("Max NPV Reached") +
  theme(panel.spacing = unit(3, "lines"))
```

In this work, we did not suffice the comparison to only Figure \@ref(fig:comp-fixbud). In Figure \@ref(fig:comp-freebud) we further allowed the number of $\overline{\mathbf{J}}(\mathbf{u})$ evaluations to 250, while keeping the results of BO to 50. Meaning that PSO and GA algorithm will enjoy another 8 iterations ($25\times8=200$) and their results, will be compared with BO. Figure \@ref(fig:comp-freebud) does not convey a single message about the performance of these methods. In \@ref(tab:comp-tab) median value of three algorithms was compared. The value in the second column of \@ref(tab:comp-tab) is the mean value of each optimization method. (In three repetitions, the maximum achieved Expected NPV is a\<b\<c, b was selected). As the \@ref(tab:comp-tab) shows, the difference between the expected NPV value of BO is almost negligible compared to PSO and GA, while the max expected NPV in BO was achieved in 50 $\overline{J}(u)$ evaluations while the other two in 250. In this work and optimization setting of the 3D, synthetic reservoir model, BO reaches the same optimal solution, while having computational complexity of 5X (times) less.

```{r comp-freebud, echo=FALSE, fig.retina=2, fig.align='center',out.width="90%", fig.cap="Comparison of GA PSO and BayesOpt performance at fixed function evaluation budget"}
ggplot(comp_data_frame, aes(Reservoir_Simulation, NPV_max, group=method)) +
  geom_point(aes(shape=method, color=method), size=2)+
  scale_shape_manual(values=c(16, 5, 0))+
  scale_color_manual(values=c('red','#E69F00', '#56B4E9'))+
  theme(legend.position="top") +
  facet_grid(cols = vars(see_number)) +
  labs(x = TeX("Number of Required, $ \\bar{J}(u)$ Evaluation")) +
  ylab("Max NPV Reached") +
  theme(panel.spacing = unit(3, "lines"))
```

```{r comp-tab, echo=FALSE, eval=TRUE}
bo_max <- c(max(bo_12345_npv_result), 
             max(bo_1234_npv_result), max(bo_123_npv_result))


pso_max <- c(max(pso_12345_npv_result), max(pso_1234_npv_result), max(pso_123_npv_result))

ga_max <- c(max(ga_12345_npv_result), max(ga_1234_npv_result), mean(ga_123_npv_result))

tibbel_analyze <- tibble(methods= c("Bayesian Optimization", "Particle Swarm Optimization", 
                                    "Genetic Alghorithm Optimization"),
                         Median_max_NPV=c(median(bo_max),median(pso_max),median(ga_max)),
                         required_simulation= c(50, 250,250))

colnames(tibbel_analyze) <- c("Optimization Method","Maximum Achieved NPV (median)", "$\\bar{m}_1$")

tibbel_analyze %>%  kableExtra::kable(format = "latex", escape = FALSE, booktabs =TRUE,align = "c", digits = 3, col.names = c("Optimization Method", "Maximum Achieved NPV (median)" ,"$\\overline{J}(u)$ Evaluations"), caption = "Summary table for comparison of GA/PSO and BO") %>% 
    kable_styling(full_width = T) %>% 
  column_spec(3, background = "blue")


```

<!-- Comparing the Final Solution $u$ of the Opt algorithms...(the Median Replication was used) -->

<!-- ```{r, echo=FALSE, fig.retina=2, fig.height=5, fig.align='center'} -->
<!-- #match(max(bo_max),bo_max) -->
<!-- bo_median <- bo_1234 -->

<!-- #match(max(pso_max),pso_max) -->
<!-- pso_median <- pso_123 -->

<!-- #match(max(ga_max),ga_max) -->
<!-- ga_median <- ga_1234 -->


<!-- vector_u_bo <-unlist(getBestPars(bo_median)) -->
<!-- vector_u_pso <- pso_median$par -->
<!-- vector_u_ga <- ga_median@solution -->


<!-- df_median_algo <- tibble(Inj=c("Inj1","Inj2","Inj3","Inj4","Inj5","Inj6","Inj7","Inj8"), -->
<!--                          BayesOpt=vector_u_bo, PSO=vector_u_pso, GA=as.numeric(vector_u_ga)) -->


<!-- df_median_algo_longer <- df_median_algo %>%  -->
<!--   pivot_longer(-Inj,names_to = "Algorithm", values_to = "Injection_Rate") -->

<!-- ggplot(data=df_median_algo_longer, aes(x=Inj, y=Injection_Rate, fill=Algorithm)) + -->
<!-- geom_bar(width = 0.4, stat="identity", position=position_dodge()) + -->
<!--   coord_cartesian(ylim = c(5, 100)) + -->
<!--   scale_color_manual(labels = c("BO", "GA", "PSO"), -->
<!--                      values = c("red", "blue", "green"), -->
<!--                      aesthetics = "fill")  -->

<!-- ``` -->
